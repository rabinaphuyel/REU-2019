{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# classifier: cats vs. dogs\n",
    "# dataset: https://www.kaggle.com/c/dogs-vs-cats\n",
    "\n",
    "# ---------------------\n",
    "# import required packages\n",
    "# ---------------------\n",
    "# import required packages\n",
    "# ---------------------\n",
    "from __future__ import print_function\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "#loading data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "#normalization\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f03ccde6710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADhZJREFUeJzt3X+MVPW5x/HPcxGIUjQQVlgpuhXNpcTkLnVCbtTc7E21UtME+kdNMSEYmyJJSS7aPzTE2E30RoK3VBMNZntZC7GlbVKsG8V7S0wjbbwhrEr4Ib1WzV5KQXaIjVgTRZbn/rGHZos735mdOTNnluf9SszOnOd89zyZ9sOZ2e+Z8zV3F4B4/qHoBgAUg/ADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwjqklYebM6cOd7V1dXKQwKhDA0N6dSpU1bLvg2F38yWSXpS0hRJ/+nuG1P7d3V1aXBwsJFDAkgolUo171v3234zmyLpaUlfl7RY0kozW1zv7wPQWo185l8q6R13f8/dz0j6uaTl+bQFoNkaCf98SX8a8/xYtu3vmNkaMxs0s8FyudzA4QDkqZHwj/dHhc99P9jd+9y95O6ljo6OBg4HIE+NhP+YpAVjnn9R0vHG2gHQKo2Ef5+k683sS2Y2TdK3JQ3k0xaAZqt7qs/dz5rZOkn/rdGpvn53P5xbZwCaqqF5fnffJWlXTr0AaCEu7wWCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCohlbpNbMhSR9JGpF01t1LeTQFNNvu3buT9bVr1ybrr732WrI+d+7cCffUag2FP/Ov7n4qh98DoIV42w8E1Wj4XdJvzOx1M1uTR0MAWqPRt/03u/txM7tS0m4z+4O77xm7Q/aPwhpJuvrqqxs8HIC8NHTmd/fj2c9hSc9LWjrOPn3uXnL3UkdHRyOHA5CjusNvZjPMbOb5x5K+JulQXo0BaK5G3vbPlfS8mZ3/PT9z9//KpSsATVd3+N39PUn/lGMvaIIPP/wwWX/55ZeT9UWLFiXr3d3dE+6pVc6ePVux1tvbmxxb7SPqZJjHr4apPiAowg8ERfiBoAg/EBThB4Ii/EBQeXyrDwUrl8sVa3fccUdy7GeffZas79mzJ1lvZwMDAxVr1b6S+9JLL+XdTtvhzA8ERfiBoAg/EBThB4Ii/EBQhB8IivADQTHPPwmcO3cuWX/ooYcq1g4dSt9f5d13303WL7/88mS9SGfOnEnWH3300Yq1a665Jjn21ltvraunyYQzPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTz/JHDgwIFkva+vr2Jtx44dybGffPJJXT21g82bNyfrb775ZsXa3r17k2OnTZtWV0+TCWd+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq6jy/mfVL+oakYXe/Ids2W9IvJHVJGpJ0p7v/pXltxvbcc88l67NmzapY6+zsTI69++67k/Ui79ufWo9ASt/HQJJ6enoq1m688cZ6Wrqo1HLm/4mkZRdse1DSK+5+vaRXsucAJpGq4Xf3PZI+uGDzcknbssfbJK3IuS8ATVbvZ/657n5CkrKfV+bXEoBWaPof/MxsjZkNmtlgtc9wAFqn3vCfNLNOScp+Dlfa0d373L3k7qWOjo46Dwcgb/WGf0DS6uzxakkv5NMOgFapGn4z2yHpfyT9o5kdM7PvSNoo6TYz+6Ok27LnACaRqvP87r6yQumrOfcS1vBwxU9NkqQnnngiWV+7dm3F2rp165Jjq30nvkhbtmxJ1kdGRpL1Rx55pGJtypQpdfV0MeEKPyAowg8ERfiBoAg/EBThB4Ii/EBQ3Lq7DRw+fDhZrzal9fTTT1eszZw5Mzm2u7s7WW+m999/P1nfuDF9+chdd92VrN9yyy0T7ikSzvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTz/G1g//79Tfvd27dvT9abfXel1DUKGzZsSI6dPn16sp76yi6q48wPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exz98G+vv7GxqfWm769ttvb+h3N+qtt96qWHv22WeTY9evX5+sX3vttXX1hFGc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqKrz/GbWL+kbkobd/YZsW6+k70oqZ7ttcPddzWpysjt9+nSyfuTIkWR93rx5yfrAwEDF2qWXXpoc22yp6wwWLlyYHLtp06a828EYtZz5fyJp2Tjbf+Tu3dl/BB+YZKqG3933SPqgBb0AaKFGPvOvM7MDZtZvZrNy6whAS9Qb/i2SFkrqlnRC0g8r7Whma8xs0MwGy+Vypd0AtFhd4Xf3k+4+4u7nJP1Y0tLEvn3uXnL3UrNvFgmgdnWF38w6xzz9pqRD+bQDoFVqmerbIalH0hwzOybpB5J6zKxbkksaknRvE3sE0ARVw+/uK8fZvLUJvVy0jh49mqyn7m0vSffdd1+yftVVV024p7ycOnUqWR8eHq5Ye/HFF5Njp06dWldPqA1X+AFBEX4gKMIPBEX4gaAIPxAU4QeC4tbdLbBgwYJk/ZJL0v8zPPXUU8n6okWLKtaq3d662tdqq30l+N5705d4LFmypGJt2bLxviyKVuHMDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc/fAldccUWy/vDDDzdUX758+YR7Ou/xxx9P1m+66aZkfdeu9I2bt26t/O3vTz/9NDl2+vTpyToaw5kfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Jinr8N3H///cn6/Pnzk/XU7bP37duXHNvb25usf/zxx8l6NatWrapYO3jwYHLsY4891tCxkcaZHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCqjrPb2YLJG2XNE/SOUl97v6kmc2W9AtJXZKGJN3p7n9pXqsXrxkzZiTr99xzT9OOvWnTpmT9gQceSNar9Za6X8Ds2bOTY9FctZz5z0r6vrt/WdI/S/qemS2W9KCkV9z9ekmvZM8BTBJVw+/uJ9z9jezxR5KOSJovabmkbdlu2yStaFaTAPI3oc/8ZtYlaYmkvZLmuvsJafQfCElX5t0cgOapOfxm9gVJv5K03t1PT2DcGjMbNLPBcrlcT48AmqCm8JvZVI0G/6fuvjPbfNLMOrN6p6Th8ca6e5+7l9y91NHRkUfPAHJQNfxmZpK2Sjri7pvHlAYkrc4er5b0Qv7tAWiWWr7Se7OkVZIOmtn+bNsGSRsl/dLMviPpqKRvNadFNGJkZCRZ37lzZ7K+ePHiZP2ZZ55J1qdOnZqsozhVw+/uv5dkFcpfzbcdAK3CFX5AUIQfCIrwA0ERfiAowg8ERfiBoLh190Xu1VdfTdb37t3b0Hjm8ScvzvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBTz/Be5gYGBhsZfd911OXWCdsOZHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYp7/IrdiRXr91F27diXrl112WZ7toI1w5geCIvxAUIQfCIrwA0ERfiAowg8ERfiBoKrO85vZAknbJc2TdE5Sn7s/aWa9kr4rqZztusHd05PGaLmenp5k/e23325NI2g7tVzkc1bS9939DTObKel1M9ud1X7k7v/RvPYANEvV8Lv7CUknsscfmdkRSfOb3RiA5prQZ34z65K0RNL5NZ7WmdkBM+s3s1kVxqwxs0EzGyyXy+PtAqAANYffzL4g6VeS1rv7aUlbJC2U1K3RdwY/HG+cu/e5e8ndSx0dHTm0DCAPNYXfzKZqNPg/dfedkuTuJ919xN3PSfqxpKXNaxNA3qqG38xM0lZJR9x985jtnWN2+6akQ/m3B6BZavlr/82SVkk6aGb7s20bJK00s25JLmlI0r1N6RBAU9Ty1/7fS7JxSszpA5MYV/gBQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCMndv3cHMypL+b8ymOZJOtayBiWnX3tq1L4ne6pVnb9e4e033y2tp+D93cLNBdy8V1kBCu/bWrn1J9FavonrjbT8QFOEHgio6/H0FHz+lXXtr174keqtXIb0V+pkfQHGKPvMDKEgh4TezZWb2v2b2jpk9WEQPlZjZkJkdNLP9ZjZYcC/9ZjZsZofGbJttZrvN7I/Zz3GXSSuot14z+3P22u03szsK6m2Bmf3WzI6Y2WEz+7dse6GvXaKvQl63lr/tN7Mpkt6WdJukY5L2SVrp7m+1tJEKzGxIUsndC58TNrN/kfRXSdvd/YZs2yZJH7j7xuwfzlnu/kCb9NYr6a9Fr9ycLSjTOXZlaUkrJN2tAl+7RF93qoDXrYgz/1JJ77j7e+5+RtLPJS0voI+25+57JH1wweblkrZlj7dp9P88LVeht7bg7ifc/Y3s8UeSzq8sXehrl+irEEWEf76kP415fkztteS3S/qNmb1uZmuKbmYcc7Nl088vn35lwf1cqOrKza10wcrSbfPa1bPidd6KCP94q/+005TDze7+FUlfl/S97O0talPTys2tMs7K0m2h3hWv81ZE+I9JWjDm+RclHS+gj3G5+/Hs57Ck59V+qw+fPL9IavZzuOB+/qadVm4eb2VptcFr104rXhcR/n2SrjezL5nZNEnfljRQQB+fY2Yzsj/EyMxmSPqa2m/14QFJq7PHqyW9UGAvf6ddVm6utLK0Cn7t2m3F60Iu8smmMp6QNEVSv7v/e8ubGIeZXavRs700uojpz4rszcx2SOrR6Le+Tkr6gaRfS/qlpKslHZX0LXdv+R/eKvTWo9G3rn9bufn8Z+wW93aLpN9JOijpXLZ5g0Y/Xxf22iX6WqkCXjeu8AOC4go/ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB/T+K8fDA+9FkXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 700 # You may select anything up to 60,000\n",
    "print(y_train[image_index]) # The label is 4\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(221)\n",
    "plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(x_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(x_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(x_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialziation\n",
    "IMG_SIZE=28\n",
    "LR=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data reshaping\n",
    "y_train=y_train.reshape((60000,1))\n",
    "x_train=x_train.reshape((60000,28,28, 1))\n",
    "x_test=x_test.reshape((10000,28,28,1))\n",
    "x_train = x_train[30000:, :]\n",
    "y_train = y_train[30000:, :]\n",
    "#x_train = np.expand_dims([x_train], axis = -1)\n",
    "#y_train = np.expand_dims([y_train], axis = -1)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 28, 28, 1)\n",
      "(30000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#checking the shape\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# define data preparation\\ndatagen = ImageDataGenerator(rotation_range = 270)\\n# fit parameters from data\\ndatagen.fit(x_train)\\n# configure batch size and retrieve one batch of images\\nfor X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\\n    # create a grid of 3x3 image\\n    for i in range(0, 9):\\n        plt.subplot(330 + 1 + i)\\n        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\\n    # show the plot\\n    plt.show()\\n    break\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# define data preparation\n",
    "datagen = ImageDataGenerator(rotation_range = 270)\n",
    "# fit parameters from data\n",
    "datagen.fit(x_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 image\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# define data preparation\\ndatagen = ImageDataGenerator(zca_whitening=True)\\n# fit parameters from data\\ndatagen.fit(x_test)\\n# configure batch size and retrieve one batch of images\\nfor X_batch, y_batch in datagen.flow(x_test, y_test, batch_size=9):\\n    # create a grid of 3x3 images\\n    for i in range(0, 9):\\n        plt.subplot(330 + 1 + i)\\n        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\\n    # show the plot\\n    plt.show()\\n    break\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# define data preparation\n",
    "datagen = ImageDataGenerator(zca_whitening=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(x_test)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(x_test, y_test, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"datagen = ImageDataGenerator(samplewise_std_normalization = True)\\n# fit parameters from data\\ndatagen.fit(x_train)\\n\\n# configure batch size and retrieve one batch of images\\nfor X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\\n    #create a grid of 3x3 images\\n    for i in range(0, 9):\\n        plt.subplot(330 + 1 + i)\\n        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\\n        # show the plot\\n        plt.show()\\n        break\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''datagen = ImageDataGenerator(samplewise_std_normalization = True)\n",
    "# fit parameters from data\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(x_train, y_train, batch_size=9):\n",
    "    #create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"datagen = ImageDataGenerator(samplewise_std_normalization = True)\\n# fit parameters from data\\ndatagen.fit(x_test)\\n\\n# configure batch size and retrieve one batch of images\\nfor X_batch, y_batch in datagen.flow(x_test, y_test, batch_size=9):\\n    #create a grid of 3x3 images\\n    for i in range(0, 9):\\n        plt.subplot(330 + 1 + i)\\n        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\\n        # show the plot\\n        plt.show()\\n        break\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''datagen = ImageDataGenerator(samplewise_std_normalization = True)\n",
    "# fit parameters from data\n",
    "datagen.fit(x_test)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(x_test, y_test, batch_size=9):\n",
    "    #create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        plt.subplot(330 + 1 + i)\n",
    "        plt.imshow(X_batch[i].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "        # show the plot\n",
    "        plt.show()\n",
    "        break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0701 10:18:05.081003 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "W0701 10:18:05.081222 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W0701 10:18:05.084760 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0701 10:18:05.087618 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0701 10:18:05.092521 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W0701 10:18:05.092792 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# construct the cnn model for this project\n",
    "# ---------------------\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "def conv_net():\n",
    "    convnet = input_data(shape = [None, IMG_SIZE, IMG_SIZE, 1],\n",
    "                        name = 'input')\n",
    "    print(convnet)\n",
    "    # conv_2d(incoming, nb_filter, filter_size, ..., activation)\n",
    "    convnet = conv_2d(convnet, 32, (3,3), activation = 'relu')\n",
    "    # max_pool_2d(incoming, kernel_size, ...)\n",
    "    convnet = max_pool_2d(convnet, 2)\n",
    "    \n",
    "\n",
    "    #flatten the layer\n",
    "    print(\"break_1\")\n",
    "    flatten = tflearn.flatten(convnet)\n",
    "    # fully_connected(incoming, n_units, activation, ...)\n",
    "    convnet = fully_connected(flatten, 5, activation = 'relu')\n",
    "    # dropout(incoming, keep_prob is drop_prob + keep+prob)\n",
    "\n",
    "    #standard recommendation for the Net arch    \n",
    "    logits = fully_connected(convnet, 10, activation = 'softmax')\n",
    "    #convnet = dropout(convnet, 0.8)\n",
    "    \n",
    "    #logits\n",
    "    #logits = tf.layers.dense(inputs=convnet, units=10)\n",
    "    \n",
    "    #regression(incoming, optimizer, learning_rate, loss, name, ...)\n",
    "    convnet = regression(logits, optimizer = 'adam', learning_rate = LR,\n",
    "                             loss = 'categorical_crossentropy', name = 'targets')\n",
    "\n",
    "    return convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0701 10:18:05.106673 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/layers/core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0701 10:18:05.109940 139655934691072 deprecation.py:506] From build/bdist.linux-x86_64/egg/tflearn/initializations.py:119: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0701 10:18:05.111963 139655934691072 deprecation.py:323] From /home/rabina7/anaconda3/envs/opencv/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:507: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "W0701 10:18:05.136765 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/layers/conv.py:552: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0701 10:18:05.141697 139655934691072 deprecation.py:506] From build/bdist.linux-x86_64/egg/tflearn/initializations.py:174: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0701 10:18:05.170104 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0701 10:18:05.179023 139655934691072 deprecation.py:506] From build/bdist.linux-x86_64/egg/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0701 10:18:05.258928 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0701 10:18:05.294300 139655934691072 deprecation.py:323] From /home/rabina7/anaconda3/envs/opencv/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.py:1250: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input/X:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "break_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0701 10:18:05.519294 139655934691072 deprecation_wrapper.py:119] From build/bdist.linux-x86_64/egg/tflearn/helpers/trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------\n",
    "# define the model\n",
    "# ---------------------\n",
    "convnet = conv_net()\n",
    "model = tflearn.DNN(convnet, tensorboard_dir = 'log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4689  | total loss: \u001b[1m\u001b[32m0.20207\u001b[0m\u001b[0m | time: 15.917s\n",
      "| Adam | epoch: 010 | loss: 0.20207 -- iter: 29952/30000\n",
      "Training Step: 4690  | total loss: \u001b[1m\u001b[32m0.18896\u001b[0m\u001b[0m | time: 15.958s\n",
      "| Adam | epoch: 010 | loss: 0.18896 -- iter: 30000/30000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(x_train,y_train,n_epoch=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.20272590e-05, 3.89038291e-07, 3.18563287e-03, ...,\n",
       "        1.17065199e-03, 2.87266797e-03, 4.18935483e-03],\n",
       "       [1.09915710e-09, 1.93134974e-06, 8.24840463e-09, ...,\n",
       "        9.99943733e-01, 3.25760077e-08, 4.79764931e-05],\n",
       "       [9.37734731e-04, 8.03838072e-12, 4.94431697e-05, ...,\n",
       "        5.44067291e-07, 1.07755500e-03, 1.76952570e-03],\n",
       "       ...,\n",
       "       [3.01795546e-04, 6.77375530e-16, 2.50942933e-09, ...,\n",
       "        8.52891773e-14, 1.48043595e-03, 2.54310929e-04],\n",
       "       [7.05651973e-06, 1.12315276e-04, 2.22116549e-04, ...,\n",
       "        2.48323531e-08, 3.21575766e-03, 4.19300632e-05],\n",
       "       [9.09750725e-06, 8.58702691e-08, 1.47938881e-05, ...,\n",
       "        3.44926541e-08, 7.11854696e-01, 4.20371629e-02]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=model.predict(x_train)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=np.argmax(p,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_y_label=np.argmax(y_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_correct_count=0\n",
    "for i in range(len(prediction)):\n",
    "    if (prediction[i]==real_y_label[i]):\n",
    "        train_correct_count=train_correct_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"train Accuracy: \"+str(train_correct_count/x_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Accuracy: 0.9422\n"
     ]
    }
   ],
   "source": [
    "test_correct_count=0;\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_test=y_test.reshape((10000,1))\n",
    "x_test=x_test.reshape((10000,28,28,1))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "p_test=model.predict(x_test);\n",
    "prediction_test=np.argmax(p_test,axis=1)\n",
    "for i in range(len(prediction_test)):\n",
    "    if (prediction_test[i]==y_test[i]):\n",
    "        test_correct_count=test_correct_count+1;\n",
    "print(\"test Accuracy: \"+str(1.0 * test_correct_count/x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [2],\n",
       "       [1],\n",
       "       ...,\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# Get current session (assuming tf backend)\n",
    "sess = K.get_session()\n",
    "# Initialize adversarial example with input image\n",
    "x_adv = x_test\n",
    "# Added noise\n",
    "x_noise = np.zeros_like(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "epochs = 10\n",
    "epsilon = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]\n",
    "target_class = 10 \n",
    "prev_probs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (10000, 1) for Tensor u'FullyConnected_1/Softmax:0', which has shape '(?, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-423c487bc259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Get the new image and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mx_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rabina7/anaconda3/envs/opencv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rabina7/anaconda3/envs/opencv/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1149\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1150\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (10000, 1) for Tensor u'FullyConnected_1/Softmax:0', which has shape '(?, 10)'"
     ]
    }
   ],
   "source": [
    "for i in range(epochs): \n",
    "    # One hot encode the target class\n",
    "    target = K.one_hot(target_class, 10)\n",
    "    \n",
    "    # Get the loss and gradient of the loss wrt the inputs\n",
    "    loss = -1*K.categorical_crossentropy(target, convnet)\n",
    "    grads = K.gradients(loss, convnet)\n",
    "\n",
    "    # Get the sign of the gradient\n",
    "    delta = K.sign(grads[0])\n",
    "    x_noise = x_noise + delta\n",
    "\n",
    "    # Perturb the image\n",
    "    x_adv = x_adv + epsilon*delta\n",
    "\n",
    "    # Get the new image and predictions\n",
    "    x_adv = sess.run(x_adv, feed_dict={convnet:x_test})\n",
    "    preds = model.predict(x_adv)\n",
    "\n",
    "    # Store the probability of the target class\n",
    "    prev_probs.append(preds[0][target_class])\n",
    "\n",
    "    if i%20==0:\n",
    "        print(i, preds[0][target_class], MNIST.decode_predictions(preds, top=10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from keras.models import load_model\n",
    "\n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend\n",
    "from keras.datasets import mnist\n",
    "from cleverhans.attacks import FastGradientMethod\n",
    "from cleverhans.attacks import BasicIterativeMethod\n",
    "from cleverhans.utils_keras import KerasModelWrapper\n",
    "backend.set_learning_phase(False)\n",
    "#(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "load_data = mnist.load_data()\n",
    "y_test=y_test.reshape((10000,1))\n",
    "x_test=x_test.reshape((10000,28,28,1))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "sess =  backend.get_session()\n",
    "wrap = KerasModelWrapper(load_data)\n",
    "fgsm = FastGradientMethod(wrap, sess=sess)\n",
    "fgsm_params = {'eps': 0.3,\n",
    "               'clip_min': 0.,\n",
    "               'clip_max': 1.}\n",
    "adv_x = fgsm.generate_np(x_test.shape[0], **fgsm_params)\n",
    "\n",
    "adv_pred = np.argmax(keras_model.predict(adv_x), axis = 1)\n",
    "adv_acc =  np.mean(np.equal(adv_pred, y_test))\n",
    "\n",
    "print(\"The adversarial validation accuracy is: {}\".format(adv_acc))''''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Define a function that stitches the 28 * 28 numpy arrays\n",
    "# together into a collage.\n",
    "def stitch_images(images, y_img_count, x_img_count, margin = 2):\n",
    "    \n",
    "    # Dimensions of the images\n",
    "    img_width = images[0].shape[0]\n",
    "    img_height = images[0].shape[1]\n",
    "    \n",
    "    width = y_img_count * img_width + (y_img_count - 1) * margin\n",
    "    height = x_img_count * img_height + (x_img_count - 1) * margin\n",
    "    stitched_images = np.zeros((width, height, 3))\n",
    "\n",
    "    # Fill the picture with our saved filters\n",
    "    for i in range(y_img_count):\n",
    "        for j in range(x_img_count):\n",
    "            img = images[i * x_img_count + j]\n",
    "            if len(img.shape) == 2:\n",
    "                img = np.dstack([img] * 3)\n",
    "            stitched_images[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                            (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "\n",
    "    return stitched_images\n",
    "\n",
    "x_sample = x_test[0].reshape(28, 28)\n",
    "adv_x_sample = adv_x[0].reshape(28, 28)\n",
    "\n",
    "adv_comparison = stitch_images([x_sample, adv_x_sample], 1, 2)\n",
    "\n",
    "plt.imshow(adv_comparison)\n",
    "plt.show()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
